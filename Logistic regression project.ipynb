{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede1fc1d",
   "metadata": {},
   "source": [
    "<h1>Deep Fake Recognition Project</h1>\n",
    "<h2>Course: DSIP</h2>\n",
    "<h2>Classification Model: Logistic Regression With Gradient Descent</h2>\n",
    "<h3>Authors: Roman Rezvin, Niv Rave</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e0d27",
   "metadata": {},
   "source": [
    "<h2>Readme</h2>\n",
    "The following project is our implementation of the Logistic Regression classification with Gradient Descent model.</br>\n",
    "The used dataset contains 1000 images sized 1024X1024, divided to 500 'Real' and 500 'Fake' images </br>\n",
    "and labled accordingly. The goal of the project is to be able to classify new images as real or fake using</br>\n",
    "the properties achieved during the learning process.</br>\n",
    "The model was built from scratch and the code contains the implementation of all the used calculations,</br>\n",
    "without any ML/AI external libraries. We used Open-CV to process the dataset images, Pandas and Numpy for</br>\n",
    "convenient storage and usage of data structures, seaborn and matplotlib.pyplot for the plots and graphical endpoints</br>\n",
    "and os for operaing system related actions.</br>\n",
    "The notebook contains different sections, each has a header that describes the following cells and their responesbility</br>\n",
    "and inside comments that describe the micro responsebilities and implementations.</br>\n",
    "The first few cells contain different utilities for plotting and data handling.</br>\n",
    "The following cells contain the Logistic Regression model's different functions and utilities, including</br>\n",
    "algorithms and optimization functions.</br>\n",
    "The last cells contain the concrete model's implementation and import/export functions to fasten and ease the</br>\n",
    "model's handling.</br>\n",
    "</br>\n",
    "<b>Regarding the model itself and the ways to initialize the model, we suggest 4 options:</b></br>\n",
    "1. Complete learning using pre-defined (optimal and efficient values we found) learning rate and iterations.</br>\n",
    "2. Complete learning with an algorithm to find the optimal learning rate and iterations.</br>\n",
    "3. Initialization through import - importing our existing model, attached in the 'model.npy' file.</br>\n",
    "4. Initialization of an empty model.</br>\n",
    "</br>\n",
    "Each initialization/creation method will affect the time to it takes to initialize, the amount of resources</br>\n",
    "the project will use in the process and the accuracy.</br>\n",
    "</br>\n",
    "The data is tagged as 'Real'=0, 'Fake'=1 and located inside matching folders in a './data' folder in the root of the</br>\n",
    "notebook (watch the matching functions' documentaion for more information).</br>\n",
    "</br>\n",
    "The calculated coefficients are found at model.w_vector for the coefficients vector (sorted by iterations)</br>\n",
    "or model.wm for the last iteration's coefficients, those to whom the model has converged. Use the import/export</br>\n",
    "functions to import/export the coefficients for later usage.</br>\n",
    "</br>\n",
    "To run a test on new data, place the data in a './data/FutureData' folder in the root of the notebook (watch the matching</br> functions' documentaion for more information).</br>\n",
    "After placing the data, activate model.test() method to view the classification results.</br>\n",
    "The results will be written to a 'FutureDataEstimatedLabels.csv' file in the root of the notebook (watch the matching</br> functions' documentaion for more information).</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec627e6",
   "metadata": {},
   "source": [
    "<h2>Imports:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7c6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sn\n",
    "import os #import os library for os location/path related methods and operations\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b18bf",
   "metadata": {},
   "source": [
    "<h2>Plot utilities</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2985d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function plots a given confusion matrix using the seaborn.heatmap() function.\n",
    "## Parameters:\n",
    "## matrix - a confusion matrix\n",
    "## color - a string, set the tiles' colors. Use 'Blues' or 'Reds' if unknown\n",
    "## data_type - a string, the data type being represented by the given matrix, used for the title.\n",
    "##             'Train','Test','Validation', etc.\n",
    "def plot_confusion_matrix(confusion_matrix,color,data_type):\n",
    "    ax = sn.heatmap(confusion_matrix, annot=True, cmap = color, cbar=False, fmt='g')\n",
    "    ax.set_title(f'{data_type} Data Confusion Matrix');\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Labled Values');\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "    plt.savefig(f'{data_type} Data Confusion Matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "## Plot the ROC curve using a confuse matrix array\n",
    "def plot_ROC(confuse_mat_array, data_type):\n",
    "    plt.plot(confuse_mat_array[:,0], confuse_mat_array[:,1], ',')\n",
    "    plt.title(f'{data_type} data ROC')\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    plt.savefig(f'{data_type} Data ROC.png')\n",
    "    plt.show()\n",
    "    \n",
    "## Plots azimuthal average of FT magnitudes of an image\n",
    "def plot_power_spectrum(power_spectrum):\n",
    "    radius_array = np.arange(0,len(np.squeeze(power_spectrum)),1) ##CHECK WHEN TRYING TO RUN\n",
    "    plt.plot(radius_array, power_spectrum, linewidth=2)\n",
    "    plt.title('Power Spectrum of an Image')\n",
    "    plt.xlabel('Spatial Frequency')\n",
    "    plt.ylabel('Power spectrum')\n",
    "    plt.show()\n",
    "\n",
    "## This function plots multiple cross-entropy plots and returns optimal iterations number and learning rate\n",
    "def check_learning_rate_array(learning_rate_arr, power, x_train, y_train, if_plot):\n",
    "    with warnings.catch_warnings():\n",
    "        # used to hide several overflow warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        # \"power\" is a power of ten, which is highest num of iterations\n",
    "        w_arr_iterations = 10**power\n",
    "        \n",
    "        # used to store cross-entropy values for plots and finding the optimal learning rate and iterations number\n",
    "        cross_entropy_mat = np.zeros((power, len(learning_rate_arr)))\n",
    "        \n",
    "        # used to loop through given learning rate array\n",
    "        lin = range(len(learning_rate_arr))\n",
    "\n",
    "        for i in lin:\n",
    "            # finds weights with given learining rate and iterations number\n",
    "            w_learn = get_coefficients(x_train, y_train, w_arr_iterations, learning_rate_arr[i])\n",
    "            \n",
    "            # stores cross-entropy value arrays\n",
    "            for j in range(0, power):\n",
    "                cross_entropy_mat[j][i] = cross_entropy(x_train, y_train, w_learn[10**(j+1) -1, :])\n",
    "                \n",
    "        if(if_plot):        \n",
    "            # the loop plots graphs and creates legend string\n",
    "            legend_string =[]\n",
    "            for j in range(0, power):\n",
    "                plt.semilogx(learning_rate_arr, cross_entropy_mat[j, :])\n",
    "                legend_string.append(str(int(10**j)) + \" iterations\")\n",
    "\n",
    "            # plot adjustments\n",
    "            plt.legend(legend_string)\n",
    "            plt.title('Cross-Entropy vs Learning Rate')\n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Cross-Entropy')\n",
    "            plt.show()\n",
    "        \n",
    "        # finds index tuple of lowes cross-entropy value\n",
    "        idx = np.unravel_index(np.argmin(cross_entropy_mat, axis=None), cross_entropy_mat.shape)\n",
    "        \n",
    "        # optimal iterations number\n",
    "        optimal_iterations_number = 10**idx[0]\n",
    "        \n",
    "        # optimal learning array\n",
    "        optimal_learning_rate = learning_rate_arr[idx[1]]\n",
    "        \n",
    "        plt.savefig('1e'+str(power)+'.png')\n",
    "        \n",
    "        return optimal_learning_rate, optimal_iterations_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89b73c",
   "metadata": {},
   "source": [
    "<h2>Dataset handling and processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dcccc",
   "metadata": {},
   "source": [
    "<h3>Classes:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d801d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataSet class\n",
    "## An initialized DataSet object gets a path to a folder containing the datasets content,\n",
    "## than calls the method get_images() which iterates through the folder and uses open-cv's .imread()\n",
    "## to import each image file in the given folder, later converting the imported dataset to a Numpy\n",
    "## array and setting the object's .images as the received array.\n",
    "class DataSet:\n",
    "    def __init__(self, path):\n",
    "        self.images = self.get_images(path)\n",
    "        \n",
    "    def get_images(self, path):\n",
    "        images = []\n",
    "        for file in os.listdir(path):\n",
    "            filename = path + '/' + os.fsdecode(file)\n",
    "            images.append(cv2.imread(filename,0))\n",
    "        return np.array(images, dtype = 'complex_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785d388",
   "metadata": {},
   "source": [
    "<h3>Functions:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482e0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return the .images property of any given DataSet object\n",
    "def get_data(data_object):\n",
    "    return data_object.images\n",
    "\n",
    "## Given a DataSet object, the method extracts the .images array, converts it\n",
    "## to a complex datatype Numpy array and iterates through each image matrix\n",
    "## in the array, calls the transform_image() and returns the new array\n",
    "def transform_matrix(data):\n",
    "    transformed_data_array = get_data(data)\n",
    "    for i in np.ndindex(transformed_data_array.shape[0]):\n",
    "        transformed_data_array[i] = transform_image(transformed_data_array[i])\n",
    "    return transformed_data_array\n",
    "\n",
    "## This method activates the fft and fftshift functions, adds 1e-8 to each calculated value\n",
    "## to avoid future math errors. The method returns the calculated DFT for each image.\n",
    "def transform_image(image):\n",
    "    transformed_image = np.fft.fft2(image)\n",
    "    transformed_image = np.fft.fftshift(transformed_image)\n",
    "    transformed_image = transformed_image + 1e-8\n",
    "    return transformed_image\n",
    "\n",
    "## Return an image's DFT's magnitude in a 20log base\n",
    "def fft_magnitude_20log_e(image):\n",
    "    return np.round(20*np.log(np.abs(image))).astype(np.uint8)\n",
    "\n",
    "## Returns normalizied matrix (for each row:(data row - row mean)/ row std ). added +1e-8 to prevent calculation errors\n",
    "def normalize_data(data):\n",
    "    return ((data-data.mean())/data.std()) + 1e-8\n",
    "\n",
    "## Returns the given type's dataset folder path (type = Real or Fake)\n",
    "def get_data_path(type):\n",
    "    return os.getcwd() + '/data/train/' + type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344b859",
   "metadata": {},
   "source": [
    "<h2>Utilities:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f58227",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The main data utility function.\n",
    "## The function gets the type of the data (We order the data in a seperate folder for each data type, \n",
    "## .../Real and .../Fake for real and fake images. Change the get_data_path function to match your own dataset location\n",
    "## according to the way you divide it), loads the image files from the given folder by creating a Data object, sending\n",
    "## the returned value of the get_data_path of the desired type. Than the data is sent to the transform_matrix to  perfrom\n",
    "## the Fourier transform related methods and we save the returned value in a data variable.\n",
    "## The function calculates the azimuthal average array by calling the get_azimuthal_average function with the\n",
    "## fft_magnitude_20log_e returned value of each data matrix (each image) and stores the results in the azimuth_average array.\n",
    "## The function returns the data, azimuth_average arrays for later usage.\n",
    "def get_data_azimuthal_average(type):\n",
    "    data = transform_matrix(DataSet(get_data_path(type)))\n",
    "\n",
    "    if (type == 'Real'):\n",
    "        export_real_data(data)\n",
    "    elif (type == 'Fake'):\n",
    "        export_fake_data(data)\n",
    "\n",
    "    azimuth_average = np.zeros((data.shape[0], np.round(data.shape[1]*np.sqrt(2)/2).astype(int) - 2))\n",
    "    for i in range(0, data.shape[0]):\n",
    "        azimuth_average[i,:]  = normalize_data(get_azimuthal_average(fft_magnitude_20log_e(data[i,:,:])))\n",
    "    data=0\n",
    "    return azimuth_average\n",
    "\n",
    "## Returns the azimuthal average for each radius.\n",
    "## The function masks the matrix by growing circles to get only the\n",
    "## values on the circle with the current radius for each radius in the matrix.\n",
    "def get_azimuthal_average(magnitudes):\n",
    "    x_center = y_center = int(magnitudes.shape[1]/2)\n",
    "    rows = magnitudes.shape[0]\n",
    "    cols= magnitudes.shape[1]\n",
    "    [X, Y] = np.meshgrid(np.arange(cols) - x_center, np.arange(rows) - y_center)\n",
    "    radius = np.sqrt(np.square(X) + np.square(Y))\n",
    "    radius_array = np.arange(1, np.max(radius), 1)\n",
    "    intensity = np.zeros(len(radius_array))\n",
    "    index = 0\n",
    "    bin_size = 1\n",
    "    for i in radius_array:\n",
    "        mask = (np.greater(radius, i - bin_size) & np.less(radius, i + bin_size))\n",
    "        values = magnitudes[mask]\n",
    "        intensity[index] = np.mean(values)\n",
    "        index += 1\n",
    "    return intensity[1:-1]\n",
    "\n",
    "## This function divides the train dataset to train and validation dataset.\n",
    "## It takes the validation data % of the entire train dataset as an argument.\n",
    "## It returns the train and validation x and y sequences.\n",
    "def divide_train_validation_data(validation_percent, real_azimuth_avg, fake_azimuth_avg):\n",
    "    offset = int(real_azimuth_avg.shape[0]*(100-validation_percent)/100)\n",
    "    x_train = np.concatenate((real_azimuth_avg[0:offset, :], fake_azimuth_avg[0:offset, :]))\n",
    "    x_validation = np.concatenate((real_azimuth_avg[offset:, :], fake_azimuth_avg[offset:, :]))\n",
    "    y_train = np.concatenate((np.zeros((1, int(x_train.shape[0]/2)), dtype = int), np.ones((1, int(x_train.shape[0]/2)), dtype = int)),axis=1)\n",
    "    y_validation = np.concatenate((np.zeros((1, int(x_validation.shape[0]/2)), dtype = int), np.ones((1, int(x_validation.shape[0]/2)), dtype = int)),axis=1)\n",
    "    return np.squeeze(x_train), np.squeeze(y_train), np.squeeze(x_validation), np.squeeze(y_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe0124",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002764f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the main Logistic Regression model function - the Gradient Descent.\n",
    "## It loops through the coefficients vector (w) for M iterations (received as input) and updates\n",
    "## the coefficients vector on each iteration using a learning rate 'a' (received as input) and performing\n",
    "## matrix multiplication between the updated logistic sigmoid and the X matrix. Both X and y are the used dataset's\n",
    "## (xn,yn) data. save_each is a boolean parameter that is used to set if each iteration's weight vector will be saved\n",
    "## (save_each==true) or just the final one\n",
    "## The returned value is the optimized coefficients vector.\n",
    "def gradient_descent_vector(M, w0, a, X, y):\n",
    "    wm = np.zeros((M, len(w0)))\n",
    "    wm[0,:] = w0\n",
    "    for i in range(1,M):\n",
    "        wm[i,:] = wm[i-1,:] - a*((sigma(wm[i-1,:], X) - y) @ X)\n",
    "    return np.squeeze(wm)\n",
    "\n",
    "## The main coefficients (weight vector, w) utility function.\n",
    "## It gets the x matrix, y vector, the number of iterations 'M' and the learning rate 'a', calculates\n",
    "## the initial weight vector (w0) by calling the calculate_coeffs function and than calculates and returns\n",
    "## the optimal weight vector by calling the Gradient Descent algorithm implemented in the gradient_descent_vector function. \n",
    "def get_coefficients(x, y, M, a):\n",
    "    w0 = np.zeros(x.shape[1])\n",
    "    wm = gradient_descent_vector(M, w0, a, x, y)\n",
    "    return wm\n",
    "    \n",
    "## This function calculates and returns the initial coefficients vector values (w0)\n",
    "def calculate_coeffs(X,t):\n",
    "    # If X is column returns scalar\n",
    "    if X.ndim == 1:\n",
    "        return ((X.T @ X)**-1)*X.T @ t.T\n",
    "    # Otherwise, returns matrix\n",
    "    return np.squeeze(np.linalg.inv(X.T @ X) @ X.T @ t.T)\n",
    "\n",
    "## This function calculates and returns the value of statistic sigmoid of train data and weights       \n",
    "def sigma(w,X):\n",
    "    with warnings.catch_warnings():\n",
    "        # used to hide several overflow warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        return np.squeeze((1/(1+np.exp(-(w @ X.T)))))\n",
    "\n",
    "## The binary prediction function, accepts a probability array of the data and a specific threshold.\n",
    "## It converts and returns an array where all values in the probability array where value > threshold are 1, else 0\n",
    "def binary_prediction(probability_array,threshold):\n",
    "    return np.squeeze(np.where(probability_array.T < threshold, 0, 1))\n",
    "\n",
    "## This function creates a confusion matrix.\n",
    "## It accepts the x matrix and y vector of each data sample and a specific threshold.\n",
    "## The matrix is returned as:\n",
    "# TP=[0,0]\n",
    "# FP=[0,1]\n",
    "# TN=[1,1]\n",
    "# FN=[1,0]\n",
    "def create_confusion_matrix(x, y, w, threshold):\n",
    "    prob_array = sigma(w,x)\n",
    "    predicted = binary_prediction(prob_array,threshold)\n",
    "    confusion_matrix = np.zeros((2,2), dtype=int)\n",
    "    \n",
    "    for idx in range(len(predicted)):\n",
    "        i=predicted[idx].astype(int)\n",
    "        j=y[idx].astype(int)\n",
    "        confusion_matrix[i,j]+=1\n",
    "    confusion_matrix = np.flip(confusion_matrix,0)\n",
    "    confusion_matrix = np.flip(confusion_matrix,1)\n",
    "    return confusion_matrix\n",
    "\n",
    "## This function calculates the model accuracy by comparing the predicted values to the labeled values.\n",
    "def accuracy(predicted, labled):\n",
    "    return np.squeeze(np.mean(labled == predicted))\n",
    "\n",
    "## This function calculates the model cross entropy value of the given traindata, test data and weights\n",
    "def cross_entropy(xn, yn, w):\n",
    "    return np.squeeze(-np.sum(yn*np.log(sigma(w, xn) + 1e-8) + (1-yn)*np.log(1 - sigma(w, xn) + 1e-8)))\n",
    "\n",
    "## This function calculates and returns the FP, TP rates and accuracy of each probability value in the\n",
    "## probability array when used as the threshold. It is used to find the optimal threshold for the classification model.\n",
    "def get_confuse_mat_array(prob_arr, x_train, y_train, wm):\n",
    "    threshold_array = prob_arr\n",
    "    confuse_mat_array=np.zeros((len(threshold_array),3))\n",
    "    for i in range(len(threshold_array)):\n",
    "        tmp_mat = create_confusion_matrix(x_train, y_train,\n",
    "                                            wm, threshold_array[i])\n",
    "        tp=tmp_mat[0,0]\n",
    "        tn=tmp_mat[1,1]\n",
    "        fn=tmp_mat[1,0]\n",
    "        fp=tmp_mat[0,1]\n",
    "        tp_rate = tp/(tp+fn)\n",
    "        fp_rate = fp/(fp+tn)\n",
    "        accuracy = (tp+tn)/(tp+tn+fn+fp)\n",
    "        confuse_mat_array[i][0]=fp_rate\n",
    "        confuse_mat_array[i][1]=tp_rate\n",
    "        confuse_mat_array[i][2]=accuracy\n",
    "    return confuse_mat_array\n",
    "\n",
    "## This function returns the value of the cell in the probability array that when used as the threshold gives the best accuracy\n",
    "## To get the most optimal threshold we loop through each value in the probability array and check the accuracy\n",
    "## using the values of the confusion matrix, using each probability as the threshold.\n",
    "def get_optimal_threshold(confuse_mat_array, prob_arr):\n",
    "    max = 0\n",
    "    maxi = 0\n",
    "    for i in range(confuse_mat_array.shape[0]):\n",
    "        if confuse_mat_array[i][2]>max:\n",
    "            max=confuse_mat_array[i][2]\n",
    "            maxi=i\n",
    "    return prob_arr[maxi]\n",
    "\n",
    "\n",
    "## This function splits the data to k chunks and runs the alogirthm to calculate the model's accuracy\n",
    "## for k iterations when each of the k chunks is used as the validation data and the other k-1 chunks\n",
    "## are used as the train data. The function returns an array with the accuracy of each iteraion.\n",
    "def k_fold_cross_validation(folds, real_azimuth_avg, fake_azimuth_avg, lr, it):    \n",
    "    #shape + 1 for label, pad with zeros for the last column of real and ones for fake\n",
    "    real = np.zeros((real_azimuth_avg.shape[0], real_azimuth_avg.shape[1] + 1))\n",
    "    real[:, 0:-1] = real_azimuth_avg\n",
    "    real[:, -1] = np.zeros(real_azimuth_avg.shape[0])\n",
    "    \n",
    "    fake = np.zeros((fake_azimuth_avg.shape[0], fake_azimuth_avg.shape[1] + 1))\n",
    "    fake[:, 0:-1] = fake_azimuth_avg\n",
    "    fake[:, -1] = np.ones(fake_azimuth_avg.shape[0])\n",
    "    \n",
    "    \n",
    "    data = np.vstack((real, fake))\n",
    "    np.random.shuffle(data)\n",
    "    folded_data = np.array(np.array_split(data, folds, axis = 0))#check if returns list\n",
    "    \n",
    "    n = len(folded_data)\n",
    "    \n",
    "    accuracy_arr = np.zeros(n)\n",
    "      \n",
    "    for i in range(n):\n",
    "        idx = np.setdiff1d(np.arange(0, n, 1, dtype=int), np.array([i]))\n",
    "        \n",
    "        train_x = np.squeeze(np.vstack(folded_data[idx, :, :-1]))\n",
    "        train_y = np.squeeze(np.hstack(folded_data[idx, :, -1]))\n",
    "                \n",
    "        validation_x = np.squeeze(np.vstack(folded_data[i, :, :-1]))\n",
    "        validation_y = np.squeeze(np.hstack(folded_data[i, :, -1]))\n",
    "                    \n",
    "        weigths = gradient_descent_vector(it, calculate_coeffs(train_x, train_y), lr, train_x, train_y)[-1]\n",
    "        \n",
    "        prob_arr = sigma(weigths, train_x)\n",
    "        \n",
    "        confuse_mat_array = get_confuse_mat_array(prob_arr, train_x, train_y, weigths)\n",
    "        \n",
    "        threshold = get_optimal_threshold(confuse_mat_array, prob_arr)\n",
    "                \n",
    "        prob_arr = sigma(weigths, validation_x)\n",
    "        \n",
    "        predicted = binary_prediction(prob_arr, threshold)\n",
    "        \n",
    "        accuracy_arr[i] = accuracy(predicted, validation_y)\n",
    "                \n",
    "    return accuracy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5ab70",
   "metadata": {},
   "source": [
    "<h2>Model import/export and initialization functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc95f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model initialize method\n",
    "def init_model(train_flag, parameters_flag):        \n",
    "    model = Model(train_flag, parameters_flag)\n",
    "    return model\n",
    "\n",
    "## Export the entire model\n",
    "def export_model_data(model):\n",
    "    with open('model.npy', 'wb') as export_file:\n",
    "        # Export data\n",
    "        np.save(export_file, model.train_data_set_x)\n",
    "        np.save(export_file, model.train_data_set_y)\n",
    "        np.save(export_file, model.validation_data_set_x)\n",
    "        np.save(export_file, model.validation_data_set_y)\n",
    "        # Export hyperparameters\n",
    "        np.save(export_file, model.iterations)\n",
    "        np.save(export_file, model.learning_rate)\n",
    "        # Export model properties\n",
    "        np.save(export_file, model.w_vector)\n",
    "        np.save(export_file, model.wm)\n",
    "        np.save(export_file, model.train_prob_array)\n",
    "        np.save(export_file, model.validation_prob_array)\n",
    "        np.save(export_file, model.threshold)\n",
    "        np.save(export_file, model.train_confusion_matrix)\n",
    "        np.save(export_file, model.validation_confusion_matrix)\n",
    "        np.save(export_file, model.train_binary_prediction)\n",
    "        np.save(export_file, model.validation_binary_prediction)\n",
    "        np.save(export_file, model.train_accuracy)\n",
    "        np.save(export_file, model.validation_accuracy)     \n",
    "\n",
    "## Export the coefficients vector - wn into .npy file\n",
    "def export_coefficients(model):\n",
    "    with open('coeffs.npy', 'wb') as export_file:\n",
    "        np.save(export_file, model.wm)\n",
    "\n",
    "## Export azimuthal average of fake data vector into .npy file\n",
    "def export_fake_azimuthal_average(data):\n",
    "    with open('fake_azimuthal_average.npy', 'wb') as export_file:\n",
    "        np.save(export_file, data)\n",
    "\n",
    "## Export azimuthal average of real data vector into .npy file       \n",
    "def export_real_azimuthal_average(data):\n",
    "    with open('real_azimuthal_average.npy', 'wb') as export_file:\n",
    "        np.save(export_file, data)\n",
    "        \n",
    "## Export both fake and real azimuthal averages into .npy file\n",
    "def export_azimuthal_average(real,fake):\n",
    "    export_fake_azimuthal_average(fake)\n",
    "    export_real_azimuthal_average(real)\n",
    "\n",
    "## Export azimuthal average of fake raw data matrix into .npy file\n",
    "def export_fake_data(data):\n",
    "    with open('fake_data.npy', 'wb') as export_file:\n",
    "        np.save(export_file, data)\n",
    "\n",
    "## Export azimuthal average of real raw data matrix into .npy file\n",
    "def export_real_data(data):\n",
    "    with open('real_data.npy', 'wb') as export_file:\n",
    "        np.save(export_file, data)  \n",
    "        \n",
    "## Export both fake and real raw data into .npy file\n",
    "def export_data(real,fake):\n",
    "    export_fake_data(fake)\n",
    "    export_real_data(real)\n",
    "    \n",
    "## Export model, both real and fake azimuthal averages and both real and fake raw data \n",
    "def fast_export(model, real, fake):\n",
    "    export_entire_model_data(model)\n",
    "    export_azimuthal_average(real,fake)\n",
    "    export_data(real,fake)\n",
    "    \n",
    "## Import an entire model and return a Model object\n",
    "def import_model_data(model):\n",
    "    with open('model.npy', 'rb') as import_file:\n",
    "        model.train_data_set_x = np.load(import_file)\n",
    "        model.train_data_set_y = np.load(import_file)\n",
    "        model.validation_data_set_x = np.load(import_file)\n",
    "        model.validation_data_set_y = np.load(import_file)\n",
    "        model.iterations = np.load(import_file)\n",
    "        model.learning_rate = np.load(import_file)\n",
    "        model.w_vector = np.load(import_file)\n",
    "        model.wm = np.load(import_file)\n",
    "        model.train_prob_array = np.load(import_file)\n",
    "        model.validation_prob_array = np.load(import_file)\n",
    "        model.threshold = np.load(import_file)\n",
    "        model.train_confusion_matrix = np.load(import_file)\n",
    "        model.validation_confusion_matrix = np.load(import_file)\n",
    "        model.train_binary_prediction = np.load(import_file)\n",
    "        model.validation_binary_prediction = np.load(import_file)\n",
    "        model.train_accuracy = np.load(import_file)\n",
    "        model.validation_accuracy = np.load(import_file)\n",
    "    return model\n",
    "\n",
    "## Import a coefficients vector - wn and return it\n",
    "def import_coefficients():\n",
    "    with open('coeffs.npy', 'rb') as import_file:\n",
    "        wm = np.load(import_file)\n",
    "    return wm\n",
    "\n",
    "## Import azimuthal average of fake data vector - fake_azimuthal_average and return it\n",
    "def import_fake_azimuthal_average():\n",
    "    with open('fake_azimuthal_average.npy', 'rb') as import_file:\n",
    "        fake = np.load(import_file)\n",
    "    return fake\n",
    "\n",
    "## Import azimuthal average of real data vector - real_azimuthal_average and return it\n",
    "def import_real_azimuthal_average():\n",
    "    with open('real_azimuthal_average.npy', 'rb') as import_file:\n",
    "        real = np.load(import_file)\n",
    "    return real\n",
    "\n",
    "## Import and return both fake and real azimuthal averages\n",
    "def import_azimuthal_average():\n",
    "    return import_fake_azimuthal_average(), import_real_azimuthal_average()\n",
    "\n",
    "## Import raw fake data matrix - fake data and return it\n",
    "def import_fake_data():\n",
    "    with open('fake_data.npy', 'rb') as import_file:\n",
    "        fake = np.load(import_file)\n",
    "    return fake \n",
    "\n",
    "## Import raw real data matrix - real data and return it     \n",
    "def import_real_data():\n",
    "    with open('real_data.npy', 'rb') as import_file:\n",
    "        real = np.load(import_file)\n",
    "    return real\n",
    "\n",
    "## Import and return both fake and real raw data\n",
    "def import_data():\n",
    "    return import_fake_data(), import_real_data()\n",
    "\n",
    "## Import model, both real and fake azimuthal averages and both real and fake raw data \n",
    "def fast_import():\n",
    "    return import_model_data(), import_azimuthal_average(), import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e733562",
   "metadata": {},
   "source": [
    "<h2>Model Optimization And Hyperparameters Tuning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a9620",
   "metadata": {},
   "source": [
    "<h4>The accuracy/iterations/lerning rates arrays are obtained from the calculated grid in the cell below the functions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71177f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function creates a grid containing the accuracy for each iterations/learning rate/data split given values.\n",
    "## The commented lines are another option to the hard-coded (manually obtained) values in the un-commented lines.\n",
    "## The function accepts a Model object as an argument and by un-commenting the 'np.savetxt' line it exports the grid to a .csv file\n",
    "def grid_search(model):\n",
    "    #learning_rate_array = np.logspace(-5, -1, num=20, base=10)\n",
    "    #interations_array = np.logspace(1, 5, num=20, base=10)\n",
    "    learning_rate_array = np.array([1e-1, 3e-1, 7e-1, 1e-2, 3e-2, 7e-2, 1e-3, 3e-3 ,7e-3, 1e-4])\n",
    "    interations_array = np.array([1e+1, 1e+2, 2e+2, 3e+2, 4e+2, 6e+2, 8e+2, 1e+3, 1.11e+3, 1.5e+3, 1e+4])\n",
    "    data_percent = np.array([10, 25, 30, 50], dtype = int)\n",
    "    grid_size = int(int(len(learning_rate_array))*int(len(interations_array))*int(len(data_percent)))\n",
    "    grid = np.zeros((grid_size,5))\n",
    "    counter = 0\n",
    "    for dp in data_percent:\n",
    "        for m in interations_array:\n",
    "            for lr in learning_rate_array:\n",
    "                model.change_validation_train_division(dp)\n",
    "                model.re_set_model(int(m),lr)\n",
    "                grid[counter] = np.array([model.validation_accuracy, 1-model.validation_accuracy, m, lr, dp])\n",
    "                counter+=1\n",
    "    #np.savetxt(\"grid_search_full_float_expanded.csv\", grid, delimiter = \",\", fmt=\"%f\", header = 'Accuracy, Error, Iterations, Learning Rate, Data Validation Percent', comments='')\n",
    "    return grid\n",
    "\n",
    "## This function calculates the model's cross entropy value of the given data, label and weights\n",
    "## x = data x matrix, y = labels, wm = weight vector\n",
    "def cross_entropy(x, y, w):\n",
    "    return np.squeeze(np.min(-np.sum(yn*np.log(sigma(w, xn) + 1e-8) + (1-yn)*np.log(1 - sigma(w, xn) + 1e-8))))\n",
    "\n",
    "## This function is used to create a graph describing the change of the Cross Entropy by the step/iteration number of the gradient descent\n",
    "## x = data x matrix, y = labels, wm = weight vector\n",
    "def graph_cost_vs_steps(x, y, wm):\n",
    "    lce = []\n",
    "    for w in wm:\n",
    "        lce.append(cross_entropy1(x,y,w))\n",
    "    plt.plot(range(len(lce)),lce)\n",
    "    plt.title('Cross entropy by Gradient Descent Step')\n",
    "    plt.xlabel('Gradient Descent Step')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.savefig('Cross Entropy by Gradient Descent step.png')\n",
    "    plt.show()\n",
    "    \n",
    "## This function calculates and creates a graph of the predictions/probabilities of given data (x).\n",
    "## data_type = 'Train' or 'Validation', y_type = 'Probability' or 'Prediction'\n",
    "def plot_predictions(x, y, data_type, y_type):\n",
    "    parameters_array = {'Predictions': y,\n",
    "                        'X': x.flatten()}\n",
    "    params = pd.DataFrame(parameters_array)\n",
    "    sn.scatterplot(x='X', y='Predictions', data=params, palette=\"deep\")\n",
    "    plt.title(f'{y_type} on {data_type} Data', y=1.015, fontsize=20)\n",
    "    plt.xlabel(f'{data_type} data features (mean)')\n",
    "    plt.ylabel(f'{y_type}');\n",
    "    plt.savefig(f'{y_type} on {data_type} Data.png')\n",
    "    \n",
    "## This function calculates and creates a graph of the accuracy calculated value by iterations for each learning rate\n",
    "## accuracy_array = calculated accuracy array, iterations_array = different iterations tested, learning_rates_array =\n",
    "## learning rates array.\n",
    "def plot_accuracy_iterations_lr_multiple(accuracy_array, iterations_array, learning_rates_array): \n",
    "    parameters_array = {'Iterations': iterations_array,\n",
    "                        'Accuracy': accuracy_array,\n",
    "                        'Learning Rate': learning_rates_array}\n",
    "    params = pd.DataFrame(parameters_array)\n",
    "    sn.relplot(x='Iterations', y='Accuracy', col='Learning Rate', col_wrap = 3, data=params, palette=\"deep\")\n",
    "    #plt.title(\"Accuracy vs Iterations per Learning Rate\", fontsize=20)\n",
    "    plt.xlabel(\"Iterations\", labelpad=13)\n",
    "    plt.ylabel(\"Accuracy\", labelpad=13)\n",
    "    plt.savefig('Accuracy for each learning rate.png')\n",
    "    ax = plt.gca()\n",
    "\n",
    "## This function calculates and creates a graph of the accuracy calculated value by iterations for all the tested learning rates\n",
    "## accuracy_array = calculated accuracy array, iterations_array = different iterations tested, learning_rates_array =\n",
    "## learning rates array.\n",
    "def plot_accuracy_iterations_lr(accuracy_array, iterations_array, learning_rates_array): \n",
    "    parameters_array = {'Iterations': iterations_array,\n",
    "                        'Accuracy': accuracy_array,\n",
    "                        'Learning Rate': learning_rates_array}\n",
    "    params = pd.DataFrame(parameters_array)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sn.scatterplot(x='Iterations', y='Accuracy', hue='Learning Rate', data=params, palette=\"deep\")\n",
    "    plt.title(\"Accuracy vs Iterations per Learning Rate\", y=1.015, fontsize=20)\n",
    "    plt.xlabel(\"Iterations\", labelpad=13)\n",
    "    plt.ylabel(\"Accuracy\", labelpad=13)\n",
    "    plt.savefig('Accuracy_vs_Iterations.png')\n",
    "    ax = plt.gca()\n",
    "    \n",
    "## This function calculates and creates a graph of the accuracy calculated value by learning rates for all the tested iterations\n",
    "## accuracy_array = calculated accuracy array, iterations_array = different iterations tested, learning_rates_array =\n",
    "## learning rates array.\n",
    "def plot_accuracy_lr_iterations(accuracy_array, iterations_array, learning_rates_array): \n",
    "    parameters_array = {'Iterations': iterations_array,\n",
    "                        'Accuracy': accuracy_array,\n",
    "                        'Learning Rate': learning_rates_array}\n",
    "    params = pd.DataFrame(parameters_array)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sn.scatterplot(x='Learning Rate', y='Accuracy', hue='Iterations', data=params, palette=\"deep\")\n",
    "    plt.title(\"Accuracy vs Learning Rate per Iterations\", y=1.015, fontsize=20)\n",
    "    plt.xlabel(\"Learning Rate\", labelpad=13)\n",
    "    plt.ylabel(\"Accuracy\", labelpad=13)\n",
    "    plt.savefig('Accuracy_vs_Learning_Rate.png')\n",
    "    ax = plt.gca()\n",
    "\n",
    "## This function calculates and creates a graph of the MSE by number of iterations\n",
    "def graph_error_vs_iterations(model):\n",
    "    mmse = np.mean((model.w_vector[1:,:] - model.w_vector[0:-1, :])**2 * (model.w_vector.shape[0]-1)-1, axis = 1)\n",
    "    plt.plot(range(len(mmse)),mmse)\n",
    "    plt.title('Error vs Iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "    print(np.argmin(mmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20356b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid creation\n",
    "#grid = grid_search(model)\n",
    "# Get the accuracy, iterations and learning rates array from the grid for several optimization and plot usage\n",
    "#accuracy_array = grid[:,0]\n",
    "#iterations_array = grid[:,2]\n",
    "#learning_rates_array = grid[:,3]\n",
    "# Convert the iteration and learning rates arrays (repeated many times) to a unique numpy array (set) for several\n",
    "# optimization and plot usage \n",
    "#iterations_set = np.unique(iterations_array)\n",
    "#learning_rates_set = np.unique(learning_rates_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0a180",
   "metadata": {},
   "source": [
    "<h2>The Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe07b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Model class containing the data for the concrete/trained logistic regression model.\n",
    "## The only argument needed to create a Model object is a boolean variable to choose between\n",
    "## training the model from scratch or importing an existing model (train_flag) and another boolean\n",
    "## variable to choose between the complete optimal learning rate and iterations process (takes a long time)\n",
    "## or using pre-defined values for both (parameters_flag).\n",
    "## A Model object can be initialized by the init_model() function or by importing a pre-trained model using the different import_ functions implemented in the cell below, each function\n",
    "## will fill the model with the desired propeties, ranging from all properties to only data/coefficients/other variations.\n",
    "## The Model has the test() method that accepts perform predictions using the model's trained properties\n",
    "## over test dataset folder/object. Read the test() documentation for more information.\n",
    "## A complete Model object holds all the needed information, parameters and data needed to perform predictions \n",
    "## and classification for new data inputs.\n",
    "class Model:\n",
    "    def __init__(self, train_flag, parameters_flag):\n",
    "        if train_flag == True:\n",
    "            self.create_model(parameters_flag)#True Any\n",
    "        elif parameters_flag == True:#False True\n",
    "            self = import_model_data(self)\n",
    "        else:#False False\n",
    "            print(\"Empty model craeted\")\n",
    "    \n",
    "    ## The model creation/initialization method. Will be called by the init_model() function to activate\n",
    "    ## the complete model training process. When finished running - the model will hold all the properties needed.\n",
    "    def create_model(self, parameters_flag):\n",
    "        \n",
    "        # Create azimuthal average from the imported data\n",
    "        real_azimuth_avg = get_data_azimuthal_average('Real')\n",
    "        fake_azimuth_avg = get_data_azimuthal_average('Fake')\n",
    "        #fake_azimuth_avg, real_azimuth_avg = import_azimuthal_average()\n",
    "        #print(\"Real azimuth shape:\")\n",
    "        #print(real_azimuth_avg.shape)                   SAY SOMETHING ABOUT IT!!!\n",
    "        #print(\"Fake azimuth shape:\")\n",
    "        #print(fake_azimuth_avg.shape)\n",
    "        export_azimuthal_average(real_azimuth_avg, fake_azimuth_avg)\n",
    "        \n",
    "        # Create and divide the data to train and validation\n",
    "        self.train_data_set_x, self.train_data_set_y, self.validation_data_set_x, self.validation_data_set_y = divide_train_validation_data(30,real_azimuth_avg, fake_azimuth_avg)\n",
    "        if parameters_flag == True:    \n",
    "            # Calculate optimal learning rate and iterations number by comparing different options and\n",
    "            # finding the minimal valid cross-entropy\n",
    "            learning_rate_arr = np.logspace(-10, -1, num=30, base=10)\n",
    "            # PAY ATTENTION! 5 is the highest recommended power for the function. \n",
    "            # Succesful run of 6 on Ryzen 5800x and 32Gb DDR4 RAM\n",
    "            # took about 2 hours. For the 7th power almost 60Gb RAM required to start the fucntion.\n",
    "            power = 5\n",
    "            self.learning_rate, self.iterations = check_learning_rate_array(learning_rate_arr, power, self.train_data_set_x, self.train_data_set_y, True)\n",
    "        else:\n",
    "            self.learning_rate = 0.1\n",
    "            self.iterations = 400\n",
    "            \n",
    "        # Perform the needed calculations, create the coefficients vector (the final iteration of the \n",
    "        # gradient descent algorithm), the probability arrays, the optimal threshold and with that value the confusion\n",
    "        # matrices and the binary predictions matrix.\n",
    "        \n",
    "        # Set the w_vector - weight vector/coefficients by calling the get_coefficients() function with\n",
    "        # the model's given data. Sets another property - wm as the final iterations' weight vector.\n",
    "        self.w_vector=get_coefficients(self.train_data_set_x, self.train_data_set_y, self.iterations, self.learning_rate)\n",
    "        self.wm=self.w_vector[-1]\n",
    "        \n",
    "        # Set the probability arrays for the train data and the validation data by calling the\n",
    "        # sigma() function with the model's given data.\n",
    "        self.train_prob_array=sigma(self.wm, self.train_data_set_x)\n",
    "        self.validation_prob_array=sigma(self.wm, self.validation_data_set_x)\n",
    "        \n",
    "        # Calculates the optimal threshold by calling the get_optimal_threshold() function with\n",
    "        # the model's given data.\n",
    "        self.threshold = get_optimal_threshold(get_confuse_mat_array(self.train_prob_array, self.train_data_set_x, self.train_data_set_y, self.wm),self.train_prob_array)\n",
    "        \n",
    "        # Generate and set the train and validation data's confusion matrices by calling the \n",
    "        # create_confusion_matrix() with the model's given data.\n",
    "        self.train_confusion_matrix = create_confusion_matrix(self.train_data_set_x, self.train_data_set_y, self.wm, self.threshold)\n",
    "        self.validation_confusion_matrix = create_confusion_matrix(self.validation_data_set_x, self.validation_data_set_y, self.wm, self.threshold)\n",
    "        \n",
    "        # Create the train and validation data's binary prediction matrices by calling the binary_prediction()\n",
    "        # function with the model's given data.\n",
    "        self.train_binary_prediction = binary_prediction(self.train_prob_array, self.threshold)\n",
    "        self.validation_binary_prediction = binary_prediction(self.validation_prob_array, self.threshold)\n",
    "        \n",
    "        # Calculate and returns the test and validation data's accuracy when using the trained model weight vector\n",
    "        # to calculate the validation data's binary prediction by calling the accuracy() function with the model's\n",
    "        # given data.\n",
    "        self.validation_accuracy = accuracy(self.validation_binary_prediction, self.validation_data_set_y)\n",
    "        self.train_accuracy = accuracy(self.train_binary_prediction, self.train_data_set_y)\n",
    "\n",
    "    def change_validation_train_division(self, percent):\n",
    "        self.train_data_set_x, self.train_data_set_y, self.validation_data_set_x, self.validation_data_set_y = divide_train_validation_data(percent, import_real_azimuthal_average(), import_fake_azimuthal_average())\n",
    "\n",
    "        \n",
    "    ## Re-set the model and calculate coefficients, probability array, throeshold and other properties\n",
    "    ## after manually updating the iterations number / learning rate value.\n",
    "    ## Accepets the new iterations number and learning rate as arguments.\n",
    "    def re_set_model(self, iterations, learning_rate):\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w_vector=get_coefficients(self.train_data_set_x, self.train_data_set_y, self.iterations, self.learning_rate)\n",
    "        self.wm=self.w_vector[-1]\n",
    "        self.train_prob_array=sigma(self.wm, self.train_data_set_x)\n",
    "        self.validation_prob_array=sigma(self.wm, self.validation_data_set_x)\n",
    "        self.threshold = get_optimal_threshold(get_confuse_mat_array(self.train_prob_array, self.train_data_set_x, self.train_data_set_y, self.wm),self.train_prob_array)\n",
    "        self.train_confusion_matrix = create_confusion_matrix(self.train_data_set_x, self.train_data_set_y, self.wm, self.threshold)\n",
    "        self.validation_confusion_matrix = create_confusion_matrix(self.validation_data_set_x, self.validation_data_set_y, self.wm, self.threshold)\n",
    "        self.train_binary_prediction = binary_prediction(self.train_prob_array, self.threshold)\n",
    "        self.validation_binary_prediction = binary_prediction(self.validation_prob_array, self.threshold)\n",
    "        self.train_accuracy = accuracy(self.train_binary_prediction, self.train_data_set_y)\n",
    "        self.validation_accuracy = accuracy(self.validation_binary_prediction, self.validation_data_set_y)\n",
    "\n",
    "    ## Export the entire model after training and defining the needed parameters to enable quick usage in the future\n",
    "    def export_model(self):\n",
    "        export_model_data(self)\n",
    "    \n",
    "    ## Calculates and returns the accuracy obtained after each iteration of the gradient descent algorithm\n",
    "    def get_accuracy_array(self):\n",
    "        accuracy_array = []\n",
    "        for i in range(self.w_vector.shape[0]):\n",
    "            confusion_matrix = create_confusion_matrix(self.validation_data_set_x, self.validation_data_set_y, self.w_vector[i], self.threshold)\n",
    "            prob_array = sigma(self.w_vector[i], self.validation_data_set_x)\n",
    "            prediction = binary_prediction(prob_array, self.threshold)\n",
    "            accuracy_array.append(accuracy(prediction, self.validation_data_set_y))\n",
    "        return accuracy_array\n",
    "            \n",
    "    ## Test function for the test (new, unseen) data.\n",
    "    ## This method imports the data, calculates the azimuthal averages of each imported image by\n",
    "    ## activating the fft algorithms and predicts whether each image is real (0) or fake (1)\n",
    "    def test(self):\n",
    "        # Import the test data from the 'data/FutureData' folder. Edit the path to match another folder if needed.\n",
    "        test = []\n",
    "        export = []\n",
    "        path = 'data/FutureData'\n",
    "        for file in os.listdir(path):\n",
    "            filename = path + '/' + os.fsdecode(file)\n",
    "            export.append(os.fsdecode(file))\n",
    "            test.append(cv2.imread(filename,0))\n",
    "        \n",
    "        # Convert to a complex data type numpy array.\n",
    "        test_arr = np.array(test, dtype = 'complex_')\n",
    "        \n",
    "        # Initialize and calculate the azimuthal averages for each image in the test data.\n",
    "        test_azimuthal_averages = np.zeros((test_arr.shape[0], 722))\n",
    "        for i in range(test_arr.shape[0]):\n",
    "            test_azimuthal_averages[i] = get_azimuthal_average(fft_magnitude_20log_e(transform_image(test_arr[i,:,:])))\n",
    "        \n",
    "        # Predict each image's label (Real = 0, Fake = 1) using the binary prediction function implemented in\n",
    "        # this model using the model's coefficients vector and threshold.\n",
    "        predict = binary_prediction(sigma(self.wm, test_azimuthal_averages), self.threshold)\n",
    "        \n",
    "        # Export each image's prediction by its file name to a '.csv' file\n",
    "        zipped = np.array(list(zip(export,predict)))\n",
    "        np.savetxt('FutureDataEstimatedLabels.csv' ,zipped, delimiter=',', header = 'File Name, Predicted', fmt = \"%s,%s\", comments='')\n",
    "        print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eec6f5",
   "metadata": {},
   "source": [
    "<h2>Main part</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee22ea4",
   "metadata": {},
   "source": [
    "<h4>Each cell below offers a different way to run the model.</h4>\n",
    "<h4>The exact results for each run type are commented in the beginning of each cell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd72278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full initialization using the optimal hyperparameters we calculated.\n",
    "## Those parameters have the best accuracy-efficieny rate.\n",
    "## Learning rate = 0.01, Iterations = 400, Data split validation:test = 25:75\n",
    "model = init_model(True, False)\n",
    "model.export_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc9f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full initialization using an algorithm to find the optimal hyperparameters.\n",
    "## This process takes a long time and uses many resources. The result is more accurate but\n",
    "## much less efficient.\n",
    "#model = init_model(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae87c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize a Model object by importing our trained model from the attached 'model.npy' file.\n",
    "## This is the quickest method to load the model.\n",
    "model2 = init_model(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be4af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize an empty Model object.\n",
    "## Use the model's methods to set or import the parameters.\n",
    "#model = init_model(False, False)\n",
    "#model = import_model_data(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
